# ==============================================================================
# GenAI-RAG-EEG Configuration File
# ==============================================================================
#
# Title: Configuration for EEG-Based Stress Classification using GenAI-RAG
# Version: 1.0.0
# Python Version: 3.8+
#
# Description:
#   Complete configuration file for the GenAI-RAG-EEG pipeline including:
#   - Model architecture parameters
#   - Training hyperparameters
#   - Data preprocessing settings
#   - RAG configuration (chunking, retrieval, generation)
#   - Evaluation metrics
#   - Hardware settings
#
# Usage:
#   python main.py --config config.yaml
#   python train.py --config config.yaml --dataset DEAP
#
# ==============================================================================

# =============================================================================
# PROJECT METADATA
# =============================================================================
project:
  name: "GenAI-RAG-EEG"
  version: "1.0.0"
  description: "Explainable EEG-Based Stress Classification using GenAI-RAG"
  authors:
    - "[Your Name]"
  license: "MIT"

# =============================================================================
# MODEL ARCHITECTURE CONFIGURATION
# =============================================================================
model:
  # Input Configuration
  n_channels: 32              # Number of EEG channels (32 for 10-20 system)
  n_time_samples: 512         # Time samples per segment (2s at 256Hz)
  n_classes: 2                # Binary classification (stress/no-stress)

  # -------------------------------------------------------------------
  # EEG ENCODER (CNN + Bi-LSTM + Attention)
  # -------------------------------------------------------------------
  # Architecture: Conv1D blocks → Bi-LSTM → Self-Attention
  # Parameters: ~138,000 trainable
  eeg_encoder:
    # Convolutional Blocks
    conv_filters: [32, 64, 64]    # Filters per block
    kernel_sizes: [7, 5, 3]        # Kernel sizes (temporal span)
    pool_sizes: [2, 2, 2]          # MaxPool sizes

    # Bi-LSTM Layer
    lstm_hidden: 64                # Hidden units per direction
    lstm_layers: 1                 # Number of LSTM layers
    lstm_dropout: 0.0              # Internal LSTM dropout

    # Self-Attention
    attention_dim: 64              # Attention intermediate dimension

    # Regularization
    dropout: 0.3                   # Conv and output dropout

    # Output
    output_dim: 128                # Feature dimension

  # -------------------------------------------------------------------
  # TEXT CONTEXT ENCODER (Sentence-BERT + Projection)
  # -------------------------------------------------------------------
  # Architecture: Frozen BERT → Trainable Projection
  # Parameters: 49,280 trainable (22.7M frozen)
  text_encoder:
    enabled: true                  # Enable/disable text encoder
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    output_dim: 128                # Match EEG encoder output
    freeze_bert: true              # Freeze pre-trained weights
    max_length: 128                # Maximum token length

  # -------------------------------------------------------------------
  # FUSION LAYER
  # -------------------------------------------------------------------
  # Combines EEG and Text features
  fusion:
    type: "concatenation"          # Options: concatenation, attention, gated
    output_dim: 128                # Fused feature dimension
    dropout: 0.3

  # -------------------------------------------------------------------
  # CLASSIFICATION HEAD
  # -------------------------------------------------------------------
  classifier:
    hidden_dims: [64, 32]          # FC layer dimensions
    dropout: 0.3

  # -------------------------------------------------------------------
  # RAG EXPLAINER
  # -------------------------------------------------------------------
  rag:
    enabled: true
    knowledge_base_path: null      # null = use default knowledge base

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  # -------------------------------------------------------------------
  # OPTIMIZATION
  # -------------------------------------------------------------------
  # Optimal hyperparameters from paper grid search
  optimizer:
    type: "AdamW"                  # Options: Adam, AdamW, SGD
    learning_rate: 1.0e-4          # Optimal: 1e-4
    weight_decay: 1.0e-2           # L2 regularization
    betas: [0.9, 0.999]
    eps: 1.0e-8

  batch_size: 64                   # Optimal: 64
  n_epochs: 100                    # Maximum epochs

  # -------------------------------------------------------------------
  # LEARNING RATE SCHEDULER
  # -------------------------------------------------------------------
  scheduler:
    type: "plateau"                # Options: plateau, cosine, step, warmup
    factor: 0.5                    # LR reduction factor
    patience: 5                    # Epochs before reduction
    min_lr: 1.0e-6                 # Minimum learning rate

  # -------------------------------------------------------------------
  # EARLY STOPPING
  # -------------------------------------------------------------------
  early_stopping:
    enabled: true
    patience: 10                   # Epochs to wait
    min_delta: 1.0e-4              # Minimum improvement
    monitor: "val_loss"            # Metric to monitor

  # -------------------------------------------------------------------
  # GRADIENT CLIPPING
  # -------------------------------------------------------------------
  gradient_clipping:
    enabled: true
    max_norm: 1.0                  # Maximum gradient norm

  # -------------------------------------------------------------------
  # MIXED PRECISION TRAINING
  # -------------------------------------------------------------------
  mixed_precision:
    enabled: true                  # Use AMP for faster training

  # -------------------------------------------------------------------
  # CHECKPOINTING
  # -------------------------------------------------------------------
  checkpoint:
    dir: "checkpoints"
    save_best: true                # Save best model
    save_last: true                # Save last epoch
    save_every: 10                 # Save every N epochs

  # -------------------------------------------------------------------
  # LOGGING
  # -------------------------------------------------------------------
  logging:
    log_interval: 10               # Log every N batches
    eval_interval: 1               # Evaluate every N epochs

  # -------------------------------------------------------------------
  # REPRODUCIBILITY
  # -------------------------------------------------------------------
  seed: 42
  deterministic: true

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  # -------------------------------------------------------------------
  # DATASET PATHS
  # -------------------------------------------------------------------
  paths:
    deap: "data/deap"
    sam40: "data/sam40"
    wesad: "data/wesad"
    sample: "data/sample"

  # -------------------------------------------------------------------
  # PREPROCESSING PIPELINE
  # -------------------------------------------------------------------
  preprocessing:
    # Bandpass Filter
    bandpass:
      enabled: true
      lowcut: 0.5                  # High-pass cutoff (Hz)
      highcut: 100.0               # Low-pass cutoff (Hz)
      order: 4                     # Butterworth filter order

    # Notch Filter (Power Line Interference)
    notch:
      enabled: true
      freq: 50.0                   # 50 Hz (Europe) or 60 Hz (US)
      quality: 30                  # Q factor

    # Artifact Removal
    artifact_removal:
      enabled: true
      method: "threshold"          # Options: ica, threshold, autoreject
      threshold: 100.0             # uV threshold

    # Segmentation
    segmentation:
      window_size: 2.0             # Seconds per segment
      overlap: 0.5                 # 50% overlap

    # Normalization
    normalization:
      enabled: true
      method: "zscore"             # Options: zscore, minmax, robust
      per_channel: true            # Per-channel normalization

  # -------------------------------------------------------------------
  # 1D TO 2D CONVERSION (Time-Frequency)
  # -------------------------------------------------------------------
  time_frequency:
    enabled: false                 # Enable for spectrogram input
    methods:
      stft:
        nperseg: 64                # Window length
        noverlap: 48               # Overlap samples
      cwt:
        wavelet: "morl"            # Morlet wavelet
        scales: 128                # Number of scales
      wvd:
        enabled: false             # Wigner-Ville Distribution

  # -------------------------------------------------------------------
  # DATA AUGMENTATION
  # -------------------------------------------------------------------
  augmentation:
    enabled: true
    techniques:
      gaussian_noise:
        enabled: true
        std: 0.01
      time_shift:
        enabled: true
        max_shift: 50              # Samples
      amplitude_scale:
        enabled: true
        range: [0.9, 1.1]

  # -------------------------------------------------------------------
  # CROSS-VALIDATION
  # -------------------------------------------------------------------
  validation:
    method: "stratified_kfold"     # Options: kfold, stratified_kfold, loso
    n_folds: 10                    # Number of folds
    test_size: 0.2                 # Held-out test set

# =============================================================================
# RAG CONFIGURATION
# =============================================================================
rag:
  enabled: true

  # -------------------------------------------------------------------
  # VECTOR DATABASE
  # -------------------------------------------------------------------
  vector_db:
    type: "faiss"                  # Options: faiss, chromadb, pinecone
    index_type: "Flat"             # Options: Flat, IVF, HNSW
    path: "data/vectordb"          # Vector store path

  # -------------------------------------------------------------------
  # CHUNKING TECHNIQUE
  # -------------------------------------------------------------------
  chunking:
    method: "semantic"             # Options: fixed, semantic, hierarchical
    chunk_size: 512                # Tokens per chunk
    chunk_overlap: 50              # Overlap between chunks

    # Semantic Chunking
    semantic:
      model: "sentence-transformers/all-MiniLM-L6-v2"
      threshold: 0.8               # Similarity threshold for splitting

  # -------------------------------------------------------------------
  # EMBEDDING
  # -------------------------------------------------------------------
  embedding:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    dimension: 384
    normalize: true
    batch_size: 32

  # -------------------------------------------------------------------
  # RETRIEVAL
  # -------------------------------------------------------------------
  retrieval:
    top_k: 5                       # Documents to retrieve
    similarity_threshold: 0.5      # Minimum similarity score

    # Pre-retrieval Processing
    pre_retrieval:
      query_expansion: true        # Expand with synonyms
      keyword_extraction: true     # Extract key terms
      rewrite: false               # LLM query rewriting

    # Post-retrieval Processing
    post_retrieval:
      reranking: true              # Re-rank with cross-encoder
      reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      context_compression: true    # Compress long contexts
      max_context_length: 2000     # Maximum context tokens

    # Hybrid Search (Dense + Sparse)
    hybrid:
      enabled: false               # Enable hybrid search
      alpha: 0.5                   # Weight: 0=sparse, 1=dense

  # -------------------------------------------------------------------
  # GENERATION
  # -------------------------------------------------------------------
  generation:
    model: "gpt-3.5-turbo"         # Options: gpt-4, gpt-3.5-turbo
    temperature: 0.3               # Lower = more deterministic
    max_tokens: 500                # Maximum response length
    top_p: 0.9

  # -------------------------------------------------------------------
  # CACHING
  # -------------------------------------------------------------------
  cache:
    enabled: true
    type: "memory"                 # Options: memory, redis, sqlite
    ttl: 3600                      # Cache TTL (seconds)
    max_size: 1000                 # Maximum entries

  # -------------------------------------------------------------------
  # KNOWLEDGE BASE
  # -------------------------------------------------------------------
  knowledge_base:
    path: "data/knowledge"
    sources:
      - "eeg_literature.txt"
      - "stress_biomarkers.txt"
      - "clinical_guidelines.txt"

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  # -------------------------------------------------------------------
  # CLASSIFICATION METRICS
  # -------------------------------------------------------------------
  metrics:
    - accuracy
    - balanced_accuracy
    - precision
    - recall
    - f1
    - auc_roc
    - auc_pr
    - specificity
    - sensitivity
    - mcc                          # Matthews Correlation Coefficient
    - cohen_kappa

  # -------------------------------------------------------------------
  # CONFIDENCE INTERVALS
  # -------------------------------------------------------------------
  confidence:
    enabled: true
    level: 0.95                    # 95% CI
    method: "bootstrap"            # Options: bootstrap, normal
    n_bootstrap: 1000

  # -------------------------------------------------------------------
  # STATISTICAL TESTS
  # -------------------------------------------------------------------
  statistical_tests:
    enabled: true
    alpha: 0.05                    # Significance level
    correction: "bonferroni"       # Multiple comparison correction
    tests:
      - paired_t_test
      - wilcoxon
      - mcnemar

  # -------------------------------------------------------------------
  # RAG EVALUATION
  # -------------------------------------------------------------------
  rag_evaluation:
    metrics:
      - faithfulness               # Groundedness in context
      - relevance                  # Answer relevance
      - context_precision          # Retrieved precision
      - context_recall             # Retrieved recall

  # -------------------------------------------------------------------
  # OUTPUT
  # -------------------------------------------------------------------
  output:
    save_predictions: true
    save_confusion_matrix: true
    save_roc_curve: true
    save_pr_curve: true
    results_dir: "results"

# =============================================================================
# HARDWARE CONFIGURATION
# =============================================================================
hardware:
  device: "auto"                   # Options: auto, cuda, cpu, mps
  gpu_ids: [0]                     # GPU device IDs for multi-GPU
  num_workers: 4                   # DataLoader workers
  pin_memory: true                 # Pin memory for faster transfer

# =============================================================================
# LOGGING & EXPERIMENT TRACKING
# =============================================================================
logging:
  level: "INFO"                    # DEBUG, INFO, WARNING, ERROR
  log_dir: "logs"
  results_dir: "results"

  # Experiment Tracking
  wandb:
    enabled: false
    project: "genai-rag-eeg"
    entity: null                   # Your W&B username

  tensorboard:
    enabled: true
    log_dir: "logs/tensorboard"

# =============================================================================
# RESPONSIBLE AI
# =============================================================================
responsible_ai:
  # Bias Detection
  bias_detection:
    enabled: true
    attributes: ["age", "gender"]

  # Safety Thresholds
  safety:
    min_confidence: 0.7            # Minimum confidence for predictions
    flag_uncertain: true           # Flag low-confidence predictions

  # Explainability
  explainability:
    enabled: true
    methods: ["attention", "gradcam"]
    save_explanations: true
